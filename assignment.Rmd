---
title: "Incomplete Data Analysis - Assignment - Nov 2019"
author: "Lorenzo Mella (UUN: s1566023)"
output: pdf_document
---


The `rmarkdown` source file that generated the present document is available at:

https://github.com/LorenzoMella/IDAassignment.git

# Problem 1

We may emphasise the distinction between the observed and unobserved variables in the vector of observations $Y$ as $Y_{\mathrm{obs}}$ and $Y_{\mathrm{mis}}$, respectively. Hence, our probabilistic model of missingness takes the form 

$$
\mathbb{P}(R_{i}=1\mid Y_{\mathrm{obs}},Y_{\mathrm{mis}},Z)=\frac{e^{Z_{i}}}{1+e^{Z_{i}}},\qquad1\le i\le n.
$$

The distribution of the missingness indicator $R$ only depends on the knowledge of the variables $Z_{i}$, not on $Y_{\mathrm{obs}}$ or $Y_{\mathrm{mis}}$. Therefore, the following identity holds:

$$
\mathbb{P}(R_{i}=1\mid Y_{\mathrm{obs}},Y_{\mathrm{mis}},Z)=\mathbb{P}(R_{i}=1\mid Z).
$$

If the data are defined as simply $Y$, the model of missingness is independent of the data. By definition, we are in an MCAR scenario. 

If, on the other hand, we take as data the complete set of variables $(Y,Z)$, we may define $X_{\mathrm{obs}} = Y_{\mathrm{obs}}$ as above, and use the notation $X_{\mathrm{mis}}=(Y_{\mathrm{mis}},Z)$ for the whole set of missing and latent variables---for completeness, we should augment the vector $R$ to include values for the missingness of the $Z$ variables too (all zeros). In this case, while the formulae for the distribution of R shown above are still valid, their interpretation differs: since the $Z_{i}$ are now considered as missing variables, the distribution of the $R_{i}$ (at least those originally given, referring to $Y$) clearly depends on them. This situation is a special case of MNAR.

Despite not knowing much about the nature of the problem, it appears that the variables $Z$ (their values being unknown) play the role of latent variables added to the model. If $Y$ had depended on $Z$ in a non-trivial way, then, the latter could have been considered as a hidden model of explanatory causes of the distribution of $Y$ and $R$. But, due to the independence of $Z$ and $Y$, restricting the attention to $Y$ as data, thus having an MCAR model of missingness, seems more natural: the $Z$ may be seen as parameters of the model of $R$ (to be arguably estimated). Or, since their number appears not to be fixed but matched to the dimension of the sample, each of the values $Z_i$ may be modelling an unrelated process of the loss of the value of $Y_i$.

<br/>

# Problem 2

We present the solution on the use of single-imputation methods applied to the `databp` dataset.

We will be using `tidyverse` to simplify most of the points.

```{r}
## Preliminary configuration
library(tidyverse)

options(pillar.sigfig = 6, pillar.neg = FALSE)

load("~/rprojects/ida/databp.Rdata")
```

\pagebreak

```{r}
# Build the tibble explicitly
# (to avoid a problem of nested variables)
bp_tb = tibble(logdose = databp$logdose,
               bloodp = databp$bloodp,
               recovtime = databp$recovtime,
               R = databp$R)
```
<br/>

The tibble has the required (original) format. We present first few lines:

```{r}
print(bp_tb)
```

### (a) Complete Case Analysis

In this section we analyse the dataset following a Complete Case Analysis strategy. We ignore, in other words, entries that present missing values on `recovtime` (patients' systolic blood pressure recovery time, the only variable with missing values in the dataset).

We first compute the mean of the complete `recovtime` entries and the mean standard error.

```{r}
# The number of complete entries
num_complete = sum(!is.na(bp_tb$recovtime))
                    
cca_stats = bp_tb %>%
  summarize(num_complete = num_complete,
            mean_rec_time = mean(recovtime, na.rm = TRUE),
            mean_std_rec_time = sd(recovtime, na.rm = TRUE) / sqrt(num_complete))

print(cca_stats)
```

We also compute the two Pearson-correlation coefficients for the recovery time and the log-dose, and the recovery time and blood pressure, respectively.

```{r}
# Pearson correlations
cca_cors = bp_tb %>%
  summarize(dose_rectime_cor = cor(logdose, recovtime, use = "complete.obs"),
            bloodp_rectime_cor = cor(bloodp, recovtime, use = "complete.obs"))

print(cca_cors)
```

### (b) Mean Imputation

We repeat the analysis using mean imputation. We fill in all missing values of `recovtime` with the average of the available ones. We retrieve the mean from the statistics computed in point (a).

```{r}
# Create a new tibble with the imputed data
mi_bp_tb = bp_tb %>%
  # Replace the NA recovtime entries with their mean value
  mutate_at(.vars = "recovtime",
            .funs = function(x) ifelse(is.na(x), cca_stats$mean_rec_time, x)) %>%
  # We remove the now useless missingness indicator from the new table
  select(-R)
```
We compute the mean of `recovtime` on the new table (this time over 25 values) and its associated standard error.

```{r}
# The number samples (complete or not),
# used here and in the following points
num_entries = nrow(bp_tb)

mi_stats = mi_bp_tb %>%
  summarize(mean_rec_time = mean(recovtime),
            mean_std_rec_time = sd(recovtime) / sqrt(num_entries))

print(mi_stats)
```

We recompute the correlations (`logdose` vs `recovtime` and `bloodp` vs `recovtime`) in the mean imputation case.

```{r}
# Compute and print Pearson correlations
mi_cors = mi_bp_tb %>%
  summarize(dose_rectime_cor = cor(logdose, recovtime),
            bloodp_rectime_cor = cor(bloodp, recovtime))

print(mi_cors)
```
\pagebreak

### (c) Mean Regression Imputation

The Mean Regression Imputation method fits a linear regression model with the `recovtime` variable as target, using the values of the complete variables as regressors. The missing values are, then, imputed using the values predicted by the linear model at the respective entry.

```{r}
# Perform linear regression
regression_data = lm(formula = recovtime ~ logdose + bloodp, data = bp_tb)
# Print formatted summary
summary(regression_data)


# Impute values as predictions using the linear model
# (all entries are predicted, for use point (e),
# but only a few of them will be used here)
linreg_pred_recovtime = bp_tb %>%
  predict(regression_data, newdata = .)

linreg_bp_tb = bp_tb %>%
  # Add column (temporarily) with predicted data to the tibble
  add_column(pred_rec_time = linreg_pred_recovtime) %>%
  # Replace NA entries in recovtime with the predicted values
  mutate(recovtime = ifelse(R == 1, recovtime, pred_rec_time)) %>%
  # Remove the temporary column and the indicator
  select(-pred_rec_time, -R)
```

We compute again mean and mean standard error.

```{r}
# Mean recovery time and std error of the mean
linreg_stats = linreg_bp_tb %>%
  summarize(mean_rec_time = mean(recovtime),
            mean_std_rec_time = sd(recovtime) / sqrt(nrow(.)))

print(linreg_stats)
```

The Pearson-correlation coefficients are as follows.

```{r}
# Pearson correlations
linreg_cors = linreg_bp_tb %>%
  summarize(dose_rectime_cor = cor(logdose, recovtime, method = "pearson"),
            bloodp_rectime_cor = cor(bloodp, recovtime, method = "pearson"))

print(linreg_cors)
```

### (d) Stochastic Regression Imputation

Mean Regression Imputation has the flaw of imputing values that all lie on the same hyperplane. A simple solution to this issue consists in "adding some jitter" to the predictions, simulating the variability of the training data.

If the linear-model hypothesis is roughly correct and the distribution of the residuals is Gaussian, we can add Gaussian noise to the predictions to mitigate the added bias in the correlations seen in the previous method. To imitate the variability in the original data, we take the standard deviation of the residuals as the corresponding parameter for the added noise.

```{r}
# Regression data inherited from point (c)
linreg_residual_sd = summary(regression_data)$sigma

# Impute values as in (c) but adding Gaussian noise
sri_pred_recovtime = linreg_pred_recovtime +
  rnorm(num_entries, mean = 0, sd = linreg_residual_sd)

sri_bp_tb = bp_tb %>%
  add_column(pred_rec_time = sri_pred_recovtime) %>%
  mutate(recovtime = ifelse(R == 1, recovtime, pred_rec_time)) %>%
  select(-pred_rec_time, -R)
```

We now recompute the mean and mean standard error.

```{r}
# Mean recovery time and std error of the mean
sri_stats = sri_bp_tb %>%
  summarize(mean_rec_time = mean(recovtime),
            mean_std_rec_time = sd(recovtime) / sqrt(sum(num_entries)))

print(sri_stats)
```

We verify that the correlations have increased.

```{r}
# Pearson correlations
sri_cors = sri_bp_tb %>%
  summarize(dose_rectime_cor = cor(logdose, recovtime, method = "pearson"),
            bloodp_rectime_cor = cor(bloodp, recovtime, method = "pearson"))

print(sri_cors)
```

As mentioned above, Stochastic Regression Imputation yields sensible results only if it is reasonable to assume a homoscedastic Gaussian residual distribution around the regression hyperplane. We perform two graphical tests of goodness of fit to verify this assumption.

The first is a plot of the studentised residuals against the regression values. If no pattern is discernible, the hypothesis of independence of the residuals should hold.

```{r}
# Linear plot of studentized residuals
sri_studentized_res = rstandard(regression_data)
plot(regression_data$fitted.values, sri_studentized_res,
     main = "Studentised residuals",
     xlab = "Fitted values (LR)", ylab = "Studentised Residuals")
```
\pagebreak

Except for an outlier, the residuals don't present noticeable patterns, especially when we remove the former, as in the graph below.

```{r}
res_no_outlier = sri_studentized_res < 2
plot(regression_data$fitted.values[res_no_outlier], sri_studentized_res[res_no_outlier],
     main = "Studentised residuals (outlier removed)",
     xlab = "Fitted values (LR)", ylab = "Studentised Residuals")
```

\pagebreak

To test for Gaussianity, we compare the ordered residuals (standardised, this time) against the theoretical quantiles of a $\mathcal{N}(0,1)$ distribution (i.e., a QQ-plot). We removed again the single outlier for clarity.

```{r}
# QQ-Plot of standardized residuals (against the theoretical Student's t distribution)
qqnorm(sri_studentized_res[res_no_outlier])
qqline(sri_studentized_res, col = 2)
```

### (e) Predictive Mean Matching

Predictive Mean Matching may be described as a hot-deck imputation method with the densest possible stratification of the data (i.e., single entries).

The predicted values of all entries (computed in point (c)) will be used to find the nearest neighbour to the missing entries. The real value at the nearest neighbour will be imputed to the respective missing variable.

```{r}
# Retrieve the missing-value indices 
missing_idx = which(is.na(bp_tb$recovtime))

# Initialise the vector of indices of the replacement entries
min_dist_idx = numeric(sum(is.na(bp_tb$recovtime)))

# Add the column of LR predicted values (from point (c)) to the table
pmm_bp_tb = bp_tb %>%
  mutate(n = 1:n()) %>%
  add_column(pred_rec_time = linreg_pred_recovtime)

# Simple utility function
sqdist = function(a, b) {
  (a - b)^2
}

# Compute the index of the minimum squared distance
# of predictions between the (available) recovtime entries
# and the missing ones
for (i in 1:length(missing_idx)) {
  min_dist_idx[i] = pmm_bp_tb %>%
    # Add a column with the squared distances
    # of the LR predicted values from the missing one
    mutate(sq_dists = sqdist(pred_rec_time, linreg_pred_recovtime[missing_idx[i]])) %>%
    # Eliminate the missing entries (we match with complete entries)
    filter(R == 1) %>%
    # Extract the index of the entry to match (minimum square distance criterion)
    slice(which.min(sq_dists)) %>%
    pull(n)
}

# Impute the values at the min-distance indices
pmm_bp_tb$recovtime[missing_idx] = pmm_bp_tb$recovtime[min_dist_idx]

# Clean the table from unnecessary columns
pmm_bp_tb = pmm_bp_tb %>% select(-pred_rec_time, -R)
```

Having imputed the values of the best matching entries to the missing values of `recovtime`, we compute mean and mean standard error and the Pearson correlations one more time.

```{r}
# Mean recovery time and std error of the mean
pmm_stats = pmm_bp_tb %>%
  summarize(mean_rec_time = mean(recovtime),
            mean_std_rec_time = sd(recovtime) / sqrt(num_entries))

print(pmm_stats)

# Pearson correlations
pmm_cors = pmm_bp_tb %>%
  summarize(dose_rectime_cor = cor(logdose, recovtime),
            bloodp_rectime_cor = cor(bloodp, recovtime))

(pmm_cors)
```

\pagebreak

### (f) Advantages and disadvantages of Predictive Mean Matching

Predictive Mean Matching is an improvement compared to Stochastic Regression Imputation: the latter makes use of the assumption that close values among the regressors (the complete variables, `logdose` and `bloodp`, in our case) correspond to close values of the predicted variable (`recovtime`). This is how we generate the missing target values. PMM doesn't impute linearly predicted values, but takes the values from among the complete cases. Doing this, the range of the values is preserved, especially for entries whose predictors have more extreme values (hence, the uncertainty on the LR predicted values is greater). We argue that, choosing from among the available values, less bias might be introduced in the dataset.

Choosing imputed values among the available data is also critical in the case when the variable with missing values is categorical (not the case in our analysis). If the categories are represented through an integer code, regression would yield real values, from which there might not be an obvious method to derive the discrete label. 

As a downside, imputed entries with close predictors will tend to have the same closest neighbour among the available data, introducing greater bias in the empirical correlations. Still referring to our scalar missing-variable case, this is particularly evident for entries whose predicted target value is beyond the maximum (or minimum) of the predicted values of the complete data. The former will all be imputed the same value.
